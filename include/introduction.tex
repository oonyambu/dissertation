
\chapter{Introduction}

\noindent



% Optimization algorithms play a crucial role in addressing complex real-world problems, where identifying optimal solutions requires efficient navigation of large, often high-dimensional search spaces. Various metaheuristic algorithms, such as Particle Swarm Optimization (PSO), Simulated Annealing (SA), Threshold Acceptance (TA), Genetic Algorithms (GA) and Differential Evolution (DE), provide the tools needed to find solutions across a variety of domains, from engineering to scientific research. Among these methods, Differential Evolution (DE) has emerged as a popular choice for continuous optimization, thanks to its simplistic nature, strong global search capabilities, robust performance and flexibility in a range of applications \parencite{storn1997differential}. In addition, DE has high convergence speed for certain problems \parencite{babu2003differential} and is parallelizable \parencite{kukkonen2006constrained}.

% However, using Differential Evolution for discrete data tasks, such as experimental design generation, presents challenges. Applying DE in such cases requires modifications to its foundational structure to effectively manage discrete variables. Recent work by \textcite{stokes2023metaheuristic} introduced a modified DE algorithm specifically tailored for these discrete tasks, highlighting the need for adaptations to extend DE's functionality beyond continuous domains. Despite these advancements, the algorithm's performance remains highly sensitive to several hyperparameters, which play a critical role in influencing outcomes.

% This sensitivity to hyperparameters emphasizes the importance of understanding and exploring the hyperparameter landscape. Developing optimized settings across different problem setups is essential for achieving reliable and efficient performance. Tailoring hyperparameters for specific tasks or datasets can greatly improve outcomes, but doing so requires a structured approach to hyperparameter tuning. As such, there is a need for strategies that systematically optimize hyperparameter configurations to align with varied and complex problem requirements. 

% In addition, Bayesian optimization has become a widely adopted approach for black-box optimization problems, particularly for its sequential nature in handling noisy and non-convex functions commonly encountered in real-world scenarios. The framework often relies on Efficient Global Optimization (EGO), a sequential strategy that iteratively searches for optimal solutions by balancing exploration of the search space and exploitation of known high-performing regions \parencite{jones1998efficient}. At each step, Bayesian optimization evaluates new points based on their potential to improve the objective function while accounting for the uncertainty of the model predictions. This iterative, or sequential, nature allows Bayesian optimization to progressively refine its estimates, focusing on promising regions and thereby enhancing efficiency.

% In Bayesian optimization, a Gaussian Process (GP) is typically used as the surrogate model, offering a flexible and powerful approach for modeling complex functions \parencite{rasmussen2006gaussian}. GPs are particularly valuable because they predict the objective function's value at unexplored points and provide an estimate of prediction uncertainty, which can guide decisions on where to sample next. The choice of acquisition function, also known as the utility function, is central to Bayesian optimization, as it defines the criteria for selecting the next sample point. Common acquisition functions include Probability of Improvement (PI), Expected Improvement (EI), Upper Confidence Bound (UCB), and Lower Confidence Bound (LCB), each designed to guide the search process differently depending on the optimization strategy.

% The Probability of Improvement (PI) function, for instance, favors points likely to outperform the current best-known solution, focusing on regions with high potential for incremental gains. Expected Improvement (EI), on the other hand, seeks points that offer the highest potential for improvement by integrating both the predicted mean and the uncertainty of the GP model, balancing exploration and exploitation more effectively \parencite{mockus1994application}. The Upper Confidence Bound (UCB) and Lower Confidence Bound (LCB) functions apply a degree of confidence to the GP predictions, where UCB emphasizes exploration by targeting areas with high uncertainty, while LCB may be used to address risk-averse or cost-sensitive problems by minimizing potential loss \parencite{srinivas2009gaussian}.

% These acquisition functions collectively enable Bayesian optimization to adapt dynamically based on the task requirements. Each function aligns with different optimization objectives, allowing Bayesian optimization to tailor its approach to complex landscapes and focus computational resources on the most promising areas of the search space. This adaptability, coupled with the sequential nature of EGO, makes Bayesian optimization a powerful method for tackling diverse and challenging optimization problems.

% In this dissertation, we employ the DE algorithm and a novel localized region shrinkage Bayesian optimization in the construction of the uniform projection designs. Three studies are done and the structure of these studies is laid as follows.

% The first study delves into the construction of space-filling designs focusing mainly on Uniform Projection Designs (UPDs) by investigating the surface structure of DE's hyperparameters and their respective contributions. {UPDs, introduced by \textcite{sun2019uniform}, are a special class of space-filling designs} that ensure an even distribution of sample points across all lower-dimensional projections of a high-dimensional design space. By analyzing the impact of hyperparameters on the performance of the modified DE algorithm, the study aims to derive optimal settings for generating {efficient UPDs} using a second-order model. Through comparisons of various experimental designs and surrogate models, the research provides crucial guidelines for enhancing DE's performance. The insights gained from this study are intended to equip practitioners with effective strategies for optimizing hyperparameter configurations in practical applications.

% Another critical aspect of optimization algorithms is the initial design choices, which can significantly impact both prediction accuracy and the efficiency of sequential optimization processes. The second study utilizes gaussian process and various initial experimental designs to model test functions determining the prediction power of the initial design. In addition, {the effect of the initial design on the optimization performance via active learning}  is evaluated. This study highlights how initial design strategies greatly influence prediction prowess and early optimization results, with their effects diminishing as iterations proceed toward the global optimum. In particular, the research demonstrates that distance-based designs, like {maximin distance designs}, struggle in high-dimensional settings, whereas Uniform Projection Designs consistently perform well across varying dimensionalities.

% In response to the challenges associated with traditional optimization approaches, the third study introduces a novel Kriging-based sequential region shrinking method, which effectively incorporates the EGO algorithm. This innovative method aims to progressively reduce the region of interest by focusing on the most promising data points during each iteration. The efficiency of this approach is validated through its application to various well-known physical test functions, where it significantly reduces the computational resources required compared to traditional hyperparameter tuning techniques like grid and random search. Furthermore, it demonstrates advantages over other Bayesian optimization strategies, such as TREGO, highlighting its practicality in resource-constrained environments.

% Together, these studies provide a comprehensive exploration of advanced optimization techniques, ranging from hyperparameter tuning in DE to effective design strategy selection, culminating in novel optimization methods. The findings contribute valuable insights to both theoretical research and practical applications, showcasing the evolving landscape of optimization strategies. By bridging the gaps in existing methodologies, this work aims to enhance the efficacy of optimization processes across diverse fields, ultimately leading to more robust and efficient solutions for complex challenges.


\section{Introduction to Optimization Algorithms }

Optimization algorithms play a pivotal role in solving complex problems where analytical solutions are either unavailable or computationally infeasible. Among these, gradient-free algorithms are particularly valuable for functions that are non-differentiable, discontinuous, or noisy. Two prominent approaches in this category are metaheuristic algorithms and Bayesian optimization, each offering unique strengths and applications.  

Metaheuristic algorithms are inspired by natural processes such as evolution, swarm behavior, or physical phenomena. Popular examples include Differential Evolution, Genetic Algorithms, Particle Swarm Optimization and Simulated Annealing. These algorithms excel in exploring vast and complex solution spaces, making them highly effective for global optimization. Their stochastic nature and population-based search strategies allow them to escape local optima and adapt to diverse problem landscapes. On the other hand, Bayesian optimization takes a probabilistic approach, using surrogate models (such as Gaussian processes) to approximate the objective function. By sequentially selecting points that balance exploration and exploitation, Bayesian optimization achieves efficiency in scenarios where function evaluations are expensive.  

Both approaches are integral to my research, offering robust, gradient-free methods to address the challenges of experimental design construction and data-driven model optimization.

\section{Metaheuristic Algorithms}

Metaheuristic algorithms are high-level problem-solving frameworks designed to find near-optimal solutions to complex optimization problems. These algorithms are particularly useful for problems where traditional optimization methods may struggle due to factors like high-dimensional search spaces, non-linearity, discontinuity, or multimodality. The key characteristics of metaheuristic algorithms include:
\begin{enumerate}
   \item \textbf{General-Purpose}: Applicable to a wide range of problems without requiring problem-specific knowledge.
   \item \textbf{Approximate Solutions}: Focus on finding good-enough solutions in a reasonable time rather than guaranteed optimal solutions.
   \item \textbf{Stochastic Nature}: Often incorporate randomization to explore the search space and avoid local optima.
   \item \textbf{Iterative Process}: Evolve candidate solutions over several iterations using a combination of exploration (searching broadly) and exploitation (refining promising areas).
\end{enumerate}

\subsection{Evolutionary Algorithms}
Evolutionary algorithms are a family of optimization techniques inspired by the principles of natural evolution, such as selection, mutation, recombination or crossover, and survival of the fittest. These algorithms operate on a population of candidate solutions, iteratively improving them based on a defined fitness function. Common examples include Genetic Algorithms (GA), Differential Evolution (DE), and Evolution Strategies (ES), each with unique mechanisms for generating and selecting solutions. 

\subsubsection{Genetic Algorithm (GA)}

Genetic Algorithms (GAs) are search heuristics inspired by the principles of natural selection and genetics, first introduced by \textcite{holland1975introductory}. In GAs, a population of candidate solutions, called chromosomes, evolves over iterations, or generations, through processes like selection, crossover, and mutation. The algorithm aims to find an optimal or near-optimal solution to a given problem by mimicking the survival-of-the-fittest principle in biological systems \parencite{mitchell1998introduction}.

The GA process begins with the initialization of a population, typically generated randomly within the solution space. Each chromosome in the population is evaluated using a fitness function, which quantifies the quality of the solution it represents. Based on fitness scores, a selection mechanism such as roulette wheel or tournament selection is used to choose parent chromosomes for reproduction. The better the fitness, the more likely a chromosome is selected, promoting better solutions \parencite{Kawachi1992goldberg}.

Reproduction involves crossover, where segments of parent chromosomes are combined to produce offspring. Common crossover methods include single-point, two-point, and uniform crossover. This recombination introduces new solution candidates into the population. Additionally, mutation is applied to offspring with a small probability to alter random genes, maintaining diversity and preventing premature convergence to local optima \parencite{de1975analysis}.

The GA process iterates through selection, crossover, mutation, and fitness evaluation until a termination criterion is met, such as a maximum number of generations or convergence to a solution. GAs have been successfully applied to various optimization problems, including scheduling, engineering design, and machine learning hyperparameter tuning. Their adaptability and ability to handle complex, non-linear problems have made them a popular choice in optimization research \parencite{whitley1994genetic}.

However, GAs face challenges like computational cost due to their population-based nature and sensitivity to parameter settings, such as mutation rate, crossover probability, and population size. Hybrid approaches combining GAs with other optimization methods, like local search or simulated annealing, have been developed to address these issues, enhancing convergence and efficiency \parencite{michalewicz2013genetic}.

\subsubsection{Differential Evolution (DE)}

Differential Evolution (DE) is a population-based optimization algorithm introduced by \textcite{storn1997differential}, specifically designed for continuous optimization problems. Unlike GAs, DE focuses on vector differences to guide its search, making it particularly effective for high-dimensional and non-linear optimization tasks. DE is simple yet powerful, with few parameters to tune \parencite{price2006differential}.

The DE process starts with initializing a population of candidate solutions as vectors within the solution space. Each vector is evaluated using an objective function to determine its fitness. The algorithm then generates trial vectors for each population member through mutation, crossover, and selection. Mutation in DE involves creating a donor vector by adding the scaled difference between two randomly chosen population vectors to a third vector, a strategy unique to DE \parencite{mezura2006comparative}.

Crossover combines the donor vector with the target vector to produce a trial vector. This process can be uniform or binomial, depending on whether crossover is controlled by a fixed or probabilistic scheme. The trial vector is evaluated, and if it outperforms the target vector in terms of fitness, it replaces the target in the population for the next generation. This selection mechanism ensures steady improvement in the population's quality over iterations \parencite{das2010differential}.

DE's strength lies in its robustness and ability to balance exploration and exploitation through its mutation and selection strategies. Its simple structure and limited parameter requirements, primarily the mutation factor and crossover rate, make it easy to implement and tune. DE has been applied to diverse areas, including function optimization, neural network training, and control system design \parencite{miettinen1999nonlinear}.

Despite its effectiveness, DE may face challenges in multimodal optimization problems, where it can become trapped in local optima. Variants of DE, such as adaptive DE and self-adaptive DE, have been developed to address these issues by dynamically adjusting parameters during the search process. Hybrid approaches integrating DE with other techniques, like gradient-based methods, have also shown promise in improving convergence and tackling complex optimization problems \parencite{qin2008differential}.



\subsubsection{Evolution Strategies (ES)}
Evolution Strategies (ES) are a prominent subclass of evolutionary algorithms tailored for optimizing real-valued continuous functions. Developed in the 1960s by Ingo Rechenberg and Hans-Paul Schwefel, these algorithms are inspired by natural evolution, focusing on adaptation and mutation while eschewing crossover mechanisms typical in other evolutionary approaches like Genetic Algorithms (\cite{vent1975rechenberg,schwefel1977numerische}). ES are population-based, relying on iterative cycles of mutation, selection, and reproduction to refine candidate solutions. Their robustness, simplicity, and adaptability make them particularly effective for solving high-dimensional, noisy, or non-linear optimization problems \parencite{beyer2002evolution}.

The standard ES workflow begins with a population of candidate solutions, each represented as a vector of real-valued parameters. Mutation, the core variation operator, introduces randomness by perturbing these parameters using Gaussian noise. Selection mechanisms, such as \((\mu+\lambda)\) or \((\mu,\lambda)\), then choose the fittest individuals to form the next generation. While $(\mu+\lambda)$ strategies retain both parents and offspring for survival, promoting stability, \((\mu,\lambda)\) strategies consider only offspring, encouraging greater exploration of the search space \parencite{schwefel1993evolution}. These mechanisms make ES versatile in balancing exploration and exploitation.

A significant extension of ES is the Directed Variation, introduced by \textcite{zhou2003directed}, which enhances the mutation process by guiding it with directional information derived from the problem landscape \parencite{zhou2003directed}. Unlike standard random isotropic mutations, directed variation biases mutation steps toward promising regions of the search space. Zhou and Li proposed frameworks for incorporating directional probabilities or fitness gradients into the mutation process, improving convergence rates and solution quality in complex optimization problems. This refinement allows ES to adapt dynamically to the fitness landscape, avoiding premature convergence while maintaining effective exploration.

The self-adaptation mechanism in ES, developed in earlier works \parencite{schwefel1981numerical}, complements directed variation by evolving strategy parameters, such as mutation step sizes, alongside candidate solutions. This approach enables the algorithm to adjust its search behavior based on the landscape's characteristics. When combined with directed variation, self-adaptation can achieve even better convergence properties by synergizing global exploration with locally informed refinement \parencite{beyer2001theory}. Directed variation thus enriches the evolutionary process by incorporating both dynamic parameter adjustment and problem-specific guidance.

Directed variation is particularly advantageous in applications involving constrained or multi-modal optimization. Zhou and Li's work highlights its efficacy in improving optimization outcomes for tasks with complex, irregular landscapes \parencite{zhou2003directed}. By biasing mutations toward beneficial directions, ES with directed variation can escape local optima and converge efficiently to global optima. This capability has been demonstrated in fields such as engineering design, robotics, and neural network training.

The theoretical foundation of directed variation draws on insights from gradient-based optimization while retaining the flexibility of stochastic methods. Zhou and Li's approach blends deterministic directional cues with stochastic exploration, bridging the gap between classical optimization techniques and evolutionary computation. This hybrid methodology underscores the broader adaptability of ES, allowing it to tackle a diverse range of optimization challenges (\cite{zhou2003directed,beyer2002evolution}).

Another notable advancement is Covariance Matrix Adaptation (CMA-ES), introduced by \textcite{hansen2001completely}. CMA-ES dynamically adjusts the covariance matrix of the mutation distribution to align with the topology of the fitness function. This technique enables ES to search more effectively along relevant dimensions of the landscape, particularly in ill-conditioned or anisotropic problems. %While CMA-ES adapts mutation in response to the population's distribution, directed variation provides an additional layer of guidance by biasing steps based on directional insights, offering a complementary approach to landscape navigation.


In summary, ES, enriched with innovations like directed variation, self-adaptation, and CMA-ES, offer a robust and versatile framework for optimization. %Directed variation, in particular, as developed by Zhou and Li, extends the capabilities of ES by introducing problem-specific directional guidance to the mutation process. 
Together, these advancements have solidified ES as a cornerstone of evolutionary computation, enabling it to excel in solving real-world problems with complex, high-dimensional, and noisy landscapes.



\subsection{Swarm intelligence (SI)} 
Swarm intelligence algorithms are optimization techniques inspired by the collective behavior of decentralized, self-organized systems in nature, such as flocks of birds, schools of fish, or ant colonies. These algorithms use the interaction of simple agents to solve complex problems without central control. Swarm intelligence models excel in parallelism, adaptability, and robustness, making them well-suited for solving optimization problems across diverse domains, including engineering, scheduling, and machine learning \parencite{kennedy1995particle}.

\subsubsection{Particle Swarm Optimization (PSO)} Introduced by \textcite{kennedy1995particle}, is inspired by the flocking behavior of birds and schooling of fish. In PSO, each individual in the swarm, called a particle, represents a candidate solution. The particles explore the solution space by updating their positions based on their own best-known position, the best-known position of their neighbors, and the overall global best position. These updates are guided by two key parameters: cognitive and social components, which balance personal learning and group influence.

The mathematical foundation of PSO lies in velocity and position updates. A particle's velocity is influenced by three components: inertia, which maintains momentum; the cognitive term, which pulls the particle toward its own best position; and the social term, which attracts the particle toward the global best position. By iteratively updating positions and velocities, PSO achieves convergence to near-optimal solutions. Its simplicity and efficiency have made it popular for a wide range of problems, from neural network training to resource allocation \parencite{clerc2002particle}.

One of the primary advantages of PSO is its ease of implementation, as it requires fewer hyperparameters compared to other optimization algorithms. However, it faces challenges such as premature convergence, especially in high-dimensional or multimodal optimization problems. Variants of PSO, such as constriction coefficient PSO and adaptive PSO, have been developed to address these limitations and improve exploration and exploitation \parencite{shi1998modified}.

\subsubsection{Ant Colony Optimization (ACO)}
Another prominent SI algorithm, was introduced by \textcite{dorigo1996ant} and is inspired by the foraging behavior of ants. Ants use pheromone trails to communicate and collectively find optimal paths between their nest and a food source. In ACO, artificial ants construct solutions by traversing a graph representation of the problem, depositing pheromones on paths to indicate their quality.

The algorithm operates iteratively, where ants build solutions based on probabilistic rules influenced by pheromone levels and heuristic information, such as edge lengths or costs. After constructing solutions, pheromones are updated: stronger paths receive more pheromone reinforcement, while weaker paths evaporate over time. This dynamic update mechanism balances exploration of new paths and exploitation of promising ones, allowing ACO to identify optimal or near-optimal solutions \parencite{dorigo2007ant}.

ACO has been successfully applied to combinatorial optimization problems, such as the Traveling Salesman Problem, vehicle routing, and scheduling. Its strengths lie in its adaptability and ability to integrate heuristic knowledge. However, ACO's performance depends on parameters like evaporation rate, pheromone influence, and heuristic weighting, which require careful tuning to avoid premature convergence or excessive exploration \parencite{blum2005ant}.

Despite their differences, both PSO and ACO share a reliance on decentralized control and emergent behavior to solve problems. While PSO excels in continuous optimization due to its particle dynamics, ACO is better suited for discrete optimization tasks because of its graph-based representation. Hybrid approaches combining PSO and ACO have been developed to leverage the strengths of both algorithms, particularly for problems with mixed discrete and continuous variables \parencite{talbi2009metaheuristics}.

Swarm intelligence algorithms, including PSO and ACO, are inherently parallelizable, making them attractive for large-scale optimization problems. Their ability to balance exploration and exploitation is a key factor in their success. Recent research has focused on adapting these algorithms to dynamic and multi-objective optimization problems, where solution landscapes change over time or involve conflicting objectives \parencite{coello2004handling}.

One challenge in swarm intelligence algorithms is their computational complexity, especially for high-dimensional problems or large populations. To address this, researchers have developed variants like distributed swarm intelligence and hybrid metaheuristics. These approaches aim to reduce computational cost while maintaining solution quality, enabling the application of swarm intelligence in fields like robotics, logistics, and bioinformatics.

Moreover, advances in hardware, such as GPUs and parallel computing platforms, have further enhanced the scalability of swarm intelligence algorithms. These developments allow for real-time applications in domains such as autonomous navigation, sensor networks, and real-time scheduling, where traditional optimization methods may fall short \parencite{yang2013swarm}.

In conclusion, swarm intelligence algorithms like PSO and ACO have revolutionized optimization by drawing inspiration from nature. Their versatility, adaptability, and robustness have enabled solutions to a wide range of complex problems. With ongoing advancements in hybridization, parameter tuning, and computational power, swarm intelligence continues to be a cornerstone of modern optimization research.



\subsection{Simulated Annealing (SA)}
Is a probabilistic optimization technique inspired by the annealing process in metallurgy, where materials are heated and slowly cooled to alter their physical properties. Proposed by \textcite{kirkpatrick1983optimization}, SA solves complex optimization problems by mimicking this physical process, exploring a solution space to find near-optimal or global optima. Unlike traditional optimization methods, it allows temporary acceptance of worse solutions to escape local minima, making it especially effective for problems with rugged landscapes.

The algorithm operates by iteratively modifying a candidate solution and evaluating its quality using a predefined objective function. A key feature of SA is its acceptance criterion, which is governed by a probabilistic function dependent on temperature and the difference in solution quality. This function, often based on the Metropolis criterion, reduces the likelihood of accepting worse solutions as the temperature decreases. This simulated "cooling" process, if managed properly, ensures convergence to an optimal solution while maintaining exploration of the solution space early on \parencite{aarts1989simulated}.

Temperature scheduling plays a critical role in SA's effectiveness. Commonly, exponential decay is used, where the temperature decreases geometrically over iterations. However, alternative cooling schedules like linear or logarithmic decay can also be employed, each with unique trade-offs in convergence speed and solution quality. The choice of cooling schedule and initial temperature significantly affects performance, as improper tuning may lead to suboptimal solutions or slow convergence.

SA has been successfully applied to a wide range of problems, including combinatorial optimization tasks like the Traveling Salesman Problem (TSP), job scheduling, and graph partitioning. For instance, \textcite{johnson1991optimization} demonstrated SA's efficiency in solving the TSP by leveraging its capability to explore and exploit the problem's complex search space. In continuous optimization, modifications to the algorithm have enabled its use in problems like parameter estimation and engineering design.

While SA has many strengths, it also faces challenges, such as the need for careful parameter tuning and the potential for slow convergence. Hybrid approaches combining SA with other optimization techniques, such as genetic algorithms or gradient-based methods, have been proposed to address these issues. Such hybrid algorithms often leverage SA's exploration capabilities while benefiting from the exploitation strengths of complementary methods \parencite{yang2010engineering}.

Theoretical work on SA has shown that it can asymptotically converge to the global optimum under certain conditions. Specifically, if the cooling schedule is sufficiently slow, the algorithm can theoretically explore all possible solutions and guarantee convergence. However, these conditions are often impractical due to computational constraints, necessitating heuristic adjustments in real-world applications \parencite{hajek1988cooling}.

In conclusion, simulated annealing remains a versatile and robust optimization technique suitable for a wide array of applications. Its success lies in its balance between exploration and exploitation, as well as its capacity to escape local minima. Advances in hybrid approaches and theoretical insights continue to expand its scope and effectiveness, making it a critical tool in modern optimization practices.


\section{Bayesian Optimization}
Beyond the metaheuristic optimization methods lies the notion of Bayesian Optimization. Bayesian optimization has become a widely adopted approach for black-box optimization problems, particularly for its sequential nature in handling noisy and non-convex functions commonly encountered in real-world scenarios. The framework often relies on Efficient Global Optimization (EGO), a sequential strategy that iteratively searches for optimal solutions by balancing exploration of the search space and exploitation of known high-performing regions \parencite{jones1998efficient}. At each step, Bayesian optimization evaluates new points based on their potential to improve the objective function while accounting for the uncertainty of the model predictions. This iterative, or sequential, nature allows Bayesian optimization to progressively refine its estimates, focusing on promising regions and thereby enhancing efficiency.

In Bayesian optimization, a Gaussian Process (GP) is typically used as the surrogate model, offering a flexible and powerful approach for modeling complex functions \parencite{rasmussen2006gaussian}. GPs are particularly valuable because they predict the objective function's value at unexplored points and provide an estimate of prediction uncertainty, which can guide decisions on where to sample next. The choice of acquisition function, also known as the utility function, is central to Bayesian optimization, as it defines the criteria for selecting the next sample point. Common acquisition functions include Probability of Improvement (PI), Expected Improvement (EI), Upper Confidence Bound (UCB), and Lower Confidence Bound (LCB), each designed to guide the search process differently depending on the optimization strategy.

The Probability of Improvement (PI) function, for instance, favors points likely to outperform the current best-known solution, focusing on regions with high potential for incremental gains. Expected Improvement (EI), on the other hand, seeks points that offer the highest potential for improvement by integrating both the predicted mean and the uncertainty of the GP model, balancing exploration and exploitation more effectively \parencite{mockus1994application}. The Upper Confidence Bound (UCB) and Lower Confidence Bound (LCB) functions apply a degree of confidence to the GP predictions, where UCB emphasizes exploration by targeting areas with high uncertainty, while LCB may be used to address risk-averse or cost-sensitive problems by minimizing potential loss \parencite{srinivas2009gaussian}.

These acquisition functions collectively enable Bayesian optimization to adapt dynamically based on the task requirements. Each function aligns with different optimization objectives, allowing Bayesian optimization to tailor its approach to complex landscapes and focus computational resources on the most promising areas of the search space. This adaptability, coupled with the sequential nature of EGO, makes Bayesian optimization a powerful method for tackling diverse and challenging optimization problems.


\section{Leveraging Differential Evolution and Bayesian Optimization}
As previously discussed, Differential Evolution (DE) is a versatile metaheuristic algorithm primarily used for optimizing continuous functions. Its simplicity and efficiency make it appealing across a wide range of applications, including experimental design. Notably, its strong global search capabilities, robust performance, and flexibility across diverse problem domains \parencite{storn1997differential}, combined with its high convergence speed for specific challenges \parencite{babu2003differential} and inherent parallelizability \parencite{kukkonen2006constrained}, have established it as a highly favorable choice for design construction.

However, using Differential Evolution for discrete data tasks, such as experimental design construction, presents challenges. Applying DE in such cases requires modifications to its foundational structure to effectively manage discrete variables and discrete search space. Recent work by \textcite{stokes2023metaheuristic}introduced a modified DE algorithm specifically tailored for these discrete tasks, highlighting the need for adaptations to extend DE's functionality beyond continuous domains. Despite these advancements, the algorithm's performance remains highly sensitive to several hyperparameters, which play a critical role in influencing outcomes.

This sensitivity to hyperparameters emphasizes the importance of understanding and exploring the hyperparameter landscape. Developing optimized settings across different problem setups is essential for achieving reliable and efficient performance. Tailoring hyperparameters for specific tasks or datasets can greatly improve outcomes, but doing so requires a structured approach to hyperparameter tuning. As such, there is a need for strategies that systematically optimize hyperparameter configurations to align with varied and complex problem requirements. 


In this dissertation, we employ the DE algorithm and a novel localized region shrinkage Bayesian optimization in the construction of the uniform projection designs. Three studies are done and the structure of these studies is laid as follows.

The first study delves into the construction of space-filling designs focusing mainly on Uniform Projection Designs (UPDs) by investigating the surface structure of DE's hyperparameters and their respective contributions. {UPDs, introduced by \textcite{sun2019uniform}, are a special class of space-filling designs} that ensure an even distribution of sample points across all lower-dimensional projections of a high-dimensional design space. By analyzing the impact of hyperparameters on the performance of the modified DE algorithm, the study aims to derive optimal settings for generating {efficient UPDs} using a second-order model. Through comparisons of various experimental designs and surrogate models, the research provides crucial guidelines for enhancing DE's performance. The insights gained from this study are intended to equip practitioners with effective strategies for optimizing hyperparameter configurations in practical applications.

Another critical aspect of optimization algorithms is the initial design choices, which can significantly impact both prediction accuracy and the efficiency of sequential optimization processes. The second study utilizes gaussian process and various initial experimental designs to model test functions determining the prediction power of the initial design. In addition, {the effect of the initial design on the optimization performance via active learning}  is evaluated. This study highlights how initial design strategies greatly influence prediction prowess and early optimization results, with their effects diminishing as iterations proceed toward the global optimum. In particular, the research demonstrates that distance-based designs, like {maximin distance designs}, struggle in high-dimensional settings, whereas Uniform Projection Designs consistently perform well across varying dimensionalities.

In response to the challenges associated with traditional optimization approaches, the third study introduces a novel Kriging-based sequential region shrinking method, which effectively incorporates the EGO algorithm. This innovative method aims to progressively reduce the region of interest by focusing on the most promising data points during each iteration. The efficiency of this approach is validated through its application to various well-known physical test functions, where it significantly reduces the computational resources required compared to traditional hyperparameter tuning techniques like grid and random search. Furthermore, it demonstrates advantages over other Bayesian optimization strategies, such as TREGO, highlighting its practicality in resource-constrained environments.

Together, these studies provide a comprehensive exploration of advanced optimization techniques, ranging from hyperparameter tuning in DE to effective design strategy selection, culminating in novel optimization methods. The findings contribute valuable insights to both theoretical research and practical applications, showcasing the evolving landscape of optimization strategies. By bridging the gaps in existing methodologies, this work aims to enhance the efficacy of optimization processes across diverse fields, ultimately leading to more robust and efficient solutions for complex challenges.

