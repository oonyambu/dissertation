%\documentclass[12pt]{article}
%\usepackage[utf8]{inputenc}
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{graphicx}
%\usepackage{tikz}
%\usepackage{hyperref}
%\usepackage{subcaption}
%\usepackage[margin=2cm]{geometry}
%\usepackage[style=authoryear, sorting=nyt, backend=bibtex, sortcites=true]{biblatex}
%\addbibresource{reference.bib}
%
%\usepackage{fullpage}
%\renewcommand{\baselinestretch}{1.25}% {1.25}
%\renewcommand{\arraystretch}{1.1}
%\renewcommand{\tabcolsep}{4pt}
%
%%% trace revision using color
%\newcommand{\reva}[1]{{\color{red} #1}}
%\newcommand{\revb}[1]{{\color{blue} #1}}
%\newcommand{\revc}[1]{{\color{cyan} #1}}
%\newcommand{\revd}[1]{{\color{blue} #1}}
%\newcommand{\reve}[1]{{\color{blue} #1}}
%%% to remove color, uncomment the following
%%\renewcommand{\reva}[1]{{#1}}  % remove color
%%\renewcommand{\revb}[1]{{#1}}
%%\renewcommand{\revc}[1]{{#1}}  % remove color
%%\renewcommand{\revd}[1]{{#1}}
%%\renewcommand{\reve}[1]{{#1}}
%
%\usepackage{amsmath,amssymb}
%\newcommand{\X}{\boldsymbol{X}}
%\newcommand{\Y}{\boldsymbol{Y}}
%\newcommand{\x}{\boldsymbol{x}}
%%\renewcommand{\u}{\boldsymbol{u}}
%\newcommand{\y}{\boldsymbol{y}}
%\newcommand{\z}{\boldsymbol{z}}
%\newcommand{\w}{\boldsymbol{w}}
%\newcommand{\R}{\boldsymbol{R}}
%\newcommand{\Rinv}{\boldsymbol{R}^{-1}}
%\newcommand{\one}{\mathbf{1}}
%\newcommand{\cov}{\textrm{Cov}}
%
%%\title{\bf A Study of Hyperparameter Tuning in a Differential Evolution Algorithm for Constructing Uniform Projection Designs}
\chapter{Tuning Differential Evolution Algorithm for Constructing Uniform Projection Designs}
%\author{Samuel Onyambu, Honquan Xu}
%\date{}
%\begin{document}
%
%\maketitle
%\begin{abstract}
%Differential Evolution (DE) is a powerful optimization algorithm inspired by the principles of natural evolution. It belongs to the class of evolutionary algorithms and has gained popularity for its simplicity, robustness, and effectiveness in solving complex optimization problems. As it works with continuous data, it has to be modified in order to work with discrete data such as design generation. Recently, \textcite{stokes2023metaheuristic} proposed such a modified DE algorithm, yet its performance heavily depends on several hyperparameters.  We aim at understanding the surface structure of the involved hyperparameters, the importance contribution of each hyperparameter, and thereby provide a guideline on optimal hyperparameter settings under different setups. We compare different types of experimental designs and surrogate models for this purpose. We illustrate the method for constructing uniform projection designs.
%\end{abstract}
%
\section{Introduction}

\SweaveOpts{concordance=FALSE, prefix=FALSE, echo=FALSE, fig=true}
<<fig=false>>=
 dr <- getwd()
 setwd('chapters/DE/')
if(!dir.exists('pdfs'))dir.create('pdfs')
@


Experimental design construction is a fundamental aspect of research and data-driven inquiry, aimed at organizing experimental runs to extract maximum information while minimizing resource use. By strategically selecting input combinations, well-constructed designs ensure that researchers can identify key factors, estimate model parameters, and predict responses accurately \parencite{montgomery2017design}. The primary goal is to balance efficiency and comprehensiveness, whether in exploring high-dimensional spaces, optimizing processes, or assessing system robustness. Central to this process is the notion of space-filling, where design points are distributed to capture variations across the entire experimental domain, providing a solid foundation for modeling and inference \parencite{santner2003design}.

Different design strategies address diverse experimental objectives and constraints. For instance, factorial and fractional factorial designs are widely used to study the main effects and interactions of factors systematically, while response surface designs, such as central composite and Box-Behnken designs, support optimization and curvature estimation \parencite{myers2016response}. In other cases, non-traditional approaches like space-filling designs (e.g., Latin hypercube or maximin designs) and discrepancy-based designs (e.g., maxpro or uniform designs) are preferred for high-dimensional and computational experiments \parencite{joseph2016space}. Each method offers unique strengths, and the choice depends on the experimental goals, computational resources, and the nature of the underlying system being studied. Through thoughtful design construction, researchers can ensure that their experiments are not only scientifically rigorous but also cost-effective and impactful.

 In the quest for robust experimental designs, Uniform Projection Designs (UPDs) have emerged as a powerful tool for ensuring uniformity across all low-dimensional projections of the design space \parencite{sun2019uniform}. UPDs, introduced by \textcite{sun2019uniform}, are specialized space-filling designs characterized by robust performance across various design criteria and impressive space-filling properties in multiple dimensions. These designs are particularly valuable in high-dimensional settings where relationships between subsets of factors often carry critical information. However, existing algorithms for generating UPDs remain underexplored, highlighting an important area for further research. To construct UPDs, we leverage the use of Differential Evolution (DE)%, an evolutionary population-based optimization algorithm, which offers a flexible and efficient approach. DE iteratively refines candidate solutions by simulating evolutionary processes like mutation, crossover, and selection (Storn & Price, 1997). By leveraging the ability of DE to handle complex, multimodal objective functions, researchers can optimize projection-based uniformity criteria, such as minimizing the discrepancy in all subspaces. This approach provides a systematic way to generate designs that balance space-filling properties across dimensions, enabling more reliable analysis and prediction in multifactor experiments.

 
%Differential Evolution (DE) is a nature-inspired optimization algorithm that mimics the process of natural selection to efficiently search for optimal solutions within large search spaces. Since its introduction in 1997, DE has gained widespread popularity and has been extensively utilized to solve complex optimization problems. Its versatility and straightforward implementation have facilitated its integration into a multitude of real-world applications, ranging from parameter tuning in machine learning algorithms to engineering design optimization and financial portfolio management. DE has consistently demonstrated its efficacy across diverse problem domains, making it a preferred choice among researchers and practitioners alike due to its adaptability to various problem structures.

%A particularly compelling application of the DE algorithm is in the realm of design generation.
Recently, \textcite{stokes2023metaheuristic} proposed a DE-based approach for constructing order-of-addition designs, showcasing its efficiency compared to other metaheuristic algorithms, such as Simulated Annealing, Threshold Accepting, Genetic Algorithms, and Particle Swarm Optimization. Inspired by their findings, we adapt and extend their modified DE algorithm for the construction of UPDs. 

The performance of the DE algorithm is significantly influenced by its hyperparameters, which dictate the learning process of the optimization \parencite{price2006differential}. An inappropriate setting of these hyperparameters can lead to suboptimal performance of the DE algorithm. While the DE algorithm proposed by \textcite{stokes2023metaheuristic} encompasses several hyperparameters, their effects remain largely unexamined. Therefore, we aim to conduct a comprehensive study of the hyperparameters to elucidate their impacts on the algorithm's performance, providing insights that could enhance its effectiveness.

The challenge of determining the optimal hyperparameter settings for any learning process has been widely studied. Two primary frameworks dominate this landscape: the model-based framework and the model-free framework. Model-based hyperparameter optimization focuses on tuning hyperparameters by approximating the true learning algorithm, while model-free methods approach the optimization problem without parametric assumptions. Relevant literature on model-based hyperparameter optimization includes works by \textcite{falkner2018bohb, hutter2011sequential, li2018hyperband, lujan2018design, mockus1978application, snoek2012practical, wu2020efficient, zoph2016neural}. In contrast, model-free frameworks encompass techniques such as manual search, grid search, random search, genetic algorithms, and orthogonal array tuning methods \parencite{liashchynskyi2019grid}.

Our approach leverages various types of designs and models to investigate the effectiveness of DE in constructing UPDs. Specifically, we aim to address three  objectives: (i) identifying useful design types in understanding the surface structure extended by the DE hyperparameters, (ii) determining the most effective models, and (iii) developing an efficient algorithm for the construction of UPDs. We employ different types of models to evaluate the performance of different designs, enabling us to visualize the response surface of the DE algorithm's hyperparameters. This framework outlines the data generation, modeling, and analysis procedures. The insights gained from our analysis provide a general guideline for optimal hyperparameter settings necessary for generating superior uniform projection designs.%The primary goal of this paper is to tune the DE algorithm for constructing UPDs through a detailed analysis of the response surface of the DE hyperparameters. This analysis involves studying how variations in hyperparameters influence the efficiency and optimality of the generated designs. By utilizing various designs to establish hyperparameter settings for DE optimization, we can identify critical factors or combinations of factors that contribute to enhanced performance. Our objective is to thoroughly explore the hyperparameter space of the DE algorithm using established designs to optimize the generation of UPDs. 
This approach is quite different from the \textcite{lujan2018design} method which used a $2^k$ factorial design with the response surface method (RSM) and ridge regression for screening to select the important factors in the data. While \textcite{lujan2018design} focuses on factor screening and selection with the traditional RSM approach, we emphasize on the comparisons of different types of designs and surrogate models in approximating the surface structure of the DE algorithm.

% \reva{(The rest can be deleted or saved for the introduction chapter)}
% In the realms of engineering, architecture, product development, and chemical mixtures, the processes of design generation and optimization are fundamental to creating innovative and efficient solutions. The designs used for the physical and computer experiments aught  to be efficient in minimizing resource use \parencite{montgomery2017design}, enhance data quality \parencite{box2005statistics}, maximize information gain \parencite{myers2016response}, optimize performance \parencite{goos2011optimal}, enhance safety, and foster innovation.

% Uniform projection designs (UPDs) are a type of space filling designs that have been shown to be robust, perform well under various design criterias, and have good space filling properties not only in two dimensions but also in all dimensions \parencite{sun2019uniform}.
% Even though advantageous, UPDs are not widely used as there is no known construction methods provided or a package to generate them. The construction of UPDs involves minimization of a given criterion. Just like any other optimization problem several meta-heuristic methods such as the Particle Swarm Optimization (PSO), Threshold Accepting (TA) algorithm , Simulated Annealing (SA), Genetic Algorithms and Differential Evolution (DE) among others could be used to tackle the task. With design construction, the data is often categorical in nature and few of these methods have been modified to carry out the optimization.  Readily available algorithm to deal with the categorical structure of the design generation process is the DE variant by \textcite{stokes2023metaheuristic}.

% Often the proposed methods involve complicated objective functions that are difficult to optimize. Most of the space filling designs are generated by optimizing a specified criterion. For example, the maximin distance design is generated by maximizing the minimum distance between any two points in the design. These optimization processes are tedious and time consuming. Methods mentioned above or other optimization methods are invoked to obtain the optimal value. For example, in the case of Maximum Projection designs \parencite{joseph2015maximum}, along with the simulated annealing algorithm, the Non-linear optimization -- NLoptr introduced by \textcite{johnson2014nlopt} is used within the MaxPro package to do the optimization. This method requires the gradient in order for the optimization to be done. In the case whereby the gradient is not provided, the method approximates the gradient using numerical methods. This by itself is computationally expensive and also unstable. In order to generate more robust designs efficiently we would employ the DE algorithm. As such, a deeper understanding of the surface structure of the DE hyperparameters is needed.


\section{Differential Evolution Algorithm}

Originating from the pioneering work of \textcite{storn1997differential}, Differential Evolution (DE) has emerged as a powerful heuristic optimization algorithm, drawing inspiration from the mechanisms of biological evolution.  To simulate survival-of-the-fittest dynamics, DE treats each candidate or agent as a chromosome made up of several genes and implements mutation and crossover procedures that allow beneficial genes to persist into future generations \parencite{storn1997differential}. DE operates on the principle of population-based search, where a set of candidate solutions evolves over successive generations towards optimal or near-optimal solutions. At its core, DE employs mutation, crossover, and selection operators to iteratively improve the quality of solutions. The algorithm's efficacy stems from its robustness, simplicity, and ability to handle non-linear, non-convex, and noisy optimization landscapes.

%Without loss of generality, we assume that we want to {maximize} a real-valued fitness function $h:\Omega\xrightarrow{}\mathbb{R}$ by finding $\pi^{*} \in \Omega$ such that \reva{$h(\pi^{*}) \ge h(\zx)$} for all $\pi \in \Omega$, where $\Omega$ is the search space with $m$ dimensions.  The search space $\Omega$ could be unconstrained like $\mathbb{R}^m$ or constrained like $[0,1]^n$ or more complicated structures.

Without loss of generality, we assume that we want to minimize a real-valued objective function $\phi$ over an $m$-dimension space $\Omega$. It has five steps

\begin{enumerate}
    \item \textbf{Genetic Representation}:
    Let $\pi_1, \ldots ,\pi_N$ be the initial population, where each agent $\pi_i = (\pi_{i1},\dots,\pi_{im})$ is randomly chosen from $\Omega$.
    \item \textbf{Mutation}:
    Mutation expands the search space of the current population.
    For each $i = 1, \ldots , N$, mutation produces a potential donor $\nu_i$ in $\Omega$ by adding the weighted difference of two agents to a third,  all randomly chosen and distinct from the target $(\pi_i)$, that is,
    \begin{equation}
        \nu_i = \pi_a + w(\pi_b - \pi_c)\\
        \label{mutation}
    \end{equation}
    where $a \neq b \neq c$ are randomly chosen three distinct numbers from $1,\dots, N$, and they are all different from $i$.
    \item \textbf{Crossover}:
    Crossover blends the current generation of agents with the population of potential donors in order to form candidates for the next generation known as trial agents. For each $i = 1,\dots, N$, one of the $m$ variables of $\nu_i$ is randomly selected to directly enter the trial agent $u_i$. In this way, one variable is forced to change so that each $u_i$ will
    certainly differ from its original target $\pi_i$. Next, with probability pCR, more variables are taken from $u_i$ and placed in the trial agent. Whichever variables do not take their value from the donor inherit their original value from $\pi_i$. Assuming $j_0$ is a random number from $1, \dots, m$, this process can be written as follows: for $j = 1,\dots, m,$
    $$
    \begin{aligned}
    u_{ij} = \begin{cases} \nu_{ij}& \text{with probability pCR}
    \text{ or if } j= j_0,\\ \pi_{ij}& \text{otherwise}\end{cases}
    \end{aligned}
    $$
    \item \textbf{Selection}:
    Selection creates the next generation of agents by comparing each target to its respective trial agent. The trial agent is adopted if it leads to an improvement and is discarded otherwise. For minimization problems, this process is given by,
    $$
    \pi_i = \begin{cases} u_i & \text{if } \phi(u_i) < \phi(\pi_i)\\
    \pi_i &\text{otherwise}  \end{cases}
    $$
    \item \textbf{Repeat}:
    Repeat steps 2 through 4 over many generations until a specified
    stopping condition is satisfied.
\end{enumerate}

Though quite simplistic, its ability to balance exploration and exploitation makes it ideal for solving non-linear and multimodal problems. 

Since experimental designs lie on a discrete and constrained space, we leverage the modified DE by \textcite{stokes2023metaheuristic}. This is to ensure that the resulting mutated design is feasible. The proposed method modifies the mutation step by using the swap mutation  \parencite{michalewicz2013genetic}, one of the structural mutation operators in genetic algorithm.  In this operator, two positions in a solution are randomly selected, and their values are exchanged. This maintains the feasibility of the solution by preserving its permutation structure while introducing variation to explore the search space. The swap mutation is computationally efficient and effective at diversifying the population, reducing the risk of premature convergence. The mutation step is controlled by mutation rate $pMut$ which is the probability of swapping two elements. In addition borrowing from PSO, they induced the influence of the global best solution with a probability $pGBest$. This yielded an algorithm which contained the following hyperparameters:
\begin{itemize}
    \item NP - The size of the population (N). % We consider values between $[10,  100]$
    \item itermax -  the maximum number of iterations/generations used. % We consider values between $[500, 1500]$
    \item pCR - Probability of crossover. %We consider values between $[0.05, 0.95]$
    \item pMut - Probability of mutation. %We consider values between $[0.05, 0.95]$
    \item pGBest - Probability of using the global best for mutation.
  %  We consider values between $[0.05, 0.95]$
    \item pSelf - Probability of using the current agent for mutation.
\end{itemize}

They proposed three different choices of the initial agent to be mutated to obtain the proposal agent. This lead to three different variants which they referred to as DE1 which uses the global best, DE2 which uses the current agent nad DE3 which uses a random agent.  

Regarding the hypermarameters, the first two, NP and itermax, determine the budget size, whereas the other four hyperparameters affect the evolution process. The hyperparameters, pGBest and pSelf, determine the probability of using the global best and the current agent in the mutation process, respectively. There is a constraint between these two hyperparameters, that is, pGBest + pSelf $\le 1$. The question that arises is how these hyperparameters interact with each other. Also whether we can do better than the proposed fixed probabilities to obtain the better settings for the DE algorithm for design generation.

In this study, we shall consider values between $[10,  100]$ for $NP$, $[500, 1500]$ for itermax, and $[0.05, 0.95]$ for pCR, pMut and pGBest. We fix $pSelf = (1 - pGBest)/2$ so that there is an equal chance for selecting a current agent and a random agent if the global best is not used.

\section{Designs for Hyperparameter Settings}\label{sec:DE.designs}
Various designs can be used to set the DE hyperparameters before the optimization process. As the functions optimized by DE are often complex with many local minima, one has to carefully choose the initial point for the optimization process. These initial points are determined using any of the methods discussed below. In each subsection below one method is described, and its benefits and drawbacks are discussed.


%Data was generated using various designs. These designs include the Full factorial design (FFD), Latin hypercube design (LHD), Uniform Projection design (UPD), maximin designs, maximum projection design (MaxPro), Central composite design (CCD) and  Orthogonal Array Composite Design (OACD).

\subsection*{Full factorial designs (FFD)}
Factorial designs are a research method for studying the effects of multiple independent variables on a response variable, formalized by Sir Ronald A. Fisher in the early 20th century \parencite{fisher1935}. These designs typically involve defining factors at two or three levels, forming a grid of all possible combinations, resulting in \(2^m\) or \(3^m\) observations for \(m\) factors. Figure \ref{fig:ffd} shows a $2^3$ full factorial design.

Full factorial designs sample points at the corners of a hypercube, ensuring uniform distribution across the design space. They allow for the analysis of main effects and interactions \parencite{montgomery2017design}, but can be complex and require larger sample sizes for adequate power \parencite{tabachnick2019}.

To mitigate the need for larger samples, fractional factorial designs were introduced by David John Finney \parencite{finney1945fractional}. These designs use a fraction of runs based on the sparsity-of-effects principle, focusing on main effects and lower-order interactions. They are expressed as \(2^{m-p}\) or \(3^{m-p}\), depending on the factors set as products of others. Selecting defining relations for fractional designs is essential, with criteria such as maximum resolution and minimum aberration guiding this process \parencite{wu2011experiments}.
\begin{figure}%[!ht]
\centering

\begin{tikzpicture}
\draw[] (0,0) -- (0,2.5);
\draw[] (0,0) -- (2.5,0);
\draw[] (2.5,0) -- (2.5,2.5);
\draw[] (0,2.5) -- (2.5,2.5);
\draw[dotted, thick] (1.5, 0.5) -- (1.5, 3);
\draw[dotted, thick] (1.5, 0.5) -- (4,0.5);
\draw[] (1.5, 3) -- (4, 3);
\draw[] (4, 0.5) -- (4, 3);
\draw[dotted, thick] (0,0) -- (1.5, 0.5);
\draw[] (0, 2.5) -- (1.5, 3);
\draw[] (2.5,0) -- (4, 0.5);
\draw[] (2.5, 2.5) -- (4, 3);
\filldraw[black] (0,0) circle(1pt);
\filldraw[black] (2.5,2.5) circle(1pt);
\filldraw[black] (0,2.5) circle(1pt);
\filldraw[black] (2.5,0) circle(1pt);
\filldraw[black] (1.5,0.5) circle(1pt);
\filldraw[black] (4,0.5) circle(1pt);
\filldraw[black] (1.5,3) circle(1pt);
\filldraw[black] (4,3) circle(1pt);

\draw[gray, |-|] (-0.01,-0.3)--(2.5,-0.3);
\node[] at (-0.01, -0.5) {${}_{\text{-}1}$};
\node[] at (2.5, -0.5) {${}_1$};
\node[] at (1.3,-0.55) {\small Factor A};

\draw[gray, |-|, rotate = 20] (2.4,-1.15)--(4,-1.15);
\node[rotate = 20] at (2.7, -.5) {${}_{\text{-}1}$};
\node[rotate=20] at (4.3, 0) {${}_1$};
\node[rotate=20] at (3.5,-0.3) {\small Factor B};

\draw[gray, |-|] (4.2, 0.5)--(4.2, 3);
\node[] at (4.5, 0.5) {${}_{\text{-}1}$};
\node[] at (4.4, 3) {${}_1$};
\node[rotate=90] at (4.4, 1.8) {\small Factor C};
\end{tikzpicture}
\caption{Geometric illustration of a $2^3$ full factorial design}\label{fig:ffd}
\end{figure}

\subsection*{Central composite designs (CCDs)}
Introduced by \textcite{box1951series} as an extension of factorial designs, they were developed as a way to efficiently fit quadratic response surfaces and identify optimal process settings in industrial experiments. CCDs are full or fractional factorial designs that are augmented with two additional sets of sampling points described as ``center'' and ``axial or star'' points \parencite{box1951series}. The center point is defined by all factors being set at their center level. The CCD uses $2m$ axial points, each of which is defined by all but one factor being at their center level and the level of the remaining factor is denoted by $\alpha$, which is generally chosen to be between 1 and $\sqrt{m}$ \parencite{montgomery2017design}. The basic concepts of the CCD for $m = 2$ and $m = 3$ are depicted in Figure \ref{fig:ccd}, where the bold dots are the design points.

\begin{figure}%[!ht]
  \centering
  %\includegraphics[height=4cm,width=8cm]{images/CCD.png}

    \begin{tikzpicture}
    \draw[] (5.2,0.2) -- (5.2, 2.3);
    \node[] at (4.5, 0) {$(-1,-1)$};

    \draw[] (5.2,0.2) -- (7.3,0.2);
    \node[] at (4.5, 2.5) {$(-1,1)$};

    \draw[] (7.3,0.2) -- (7.3, 2.3);
    \node[] at (8, 0) {$(1,-1)$};

    \draw[] (5.2,2.3) -- (7.3,2.3);
    \node[] at (8,2.5) {$(1,1)$};

    \node[] at (6.9,1.5) {$x_1$};
    \node[] at (6.5, 2) {$x_2$};

    \draw[dotted, thick] (4.7, 1.25) -- (7.8, 1.25);
    \filldraw[black] (4.7, 1.25) circle(1pt);
    \node[] at (4, 1.25) {$(-\alpha, 0)$};

    \filldraw[black] (7.8, 1.25) circle(1pt);
    \node[] at (8.3, 1.25) {$(\alpha, 0)$};

    \draw[dotted, thick] (6.25, -0.3) -- (6.25, 2.8);
    \filldraw[black] (6.25, -0.3) circle(1pt);
    \node[] at (6.25, -0.8) {$(0,-\alpha)$};

    \filldraw[black] (6.25, 2.8) circle(1pt);
    \node[] at (6.25, 3.3) {$(0,\alpha)$};

    \filldraw[color=black!60, fill=white](6.25,1.25) circle (5pt);
    \filldraw[color=black!60, fill=white](6.25,1.25) circle (3pt);
    \filldraw[black](6.25,1.25) circle (1pt);

    \draw[] (10,0) -- (10, 2.5);
    \draw[] (10,0) -- (12.5,0);
    \draw[] (12.5,0) -- (12.5,2.5);
    \draw[] (10,2.5) -- (12.5,2.5);
    \draw[dotted, thick] (11.5, 0.5) -- (11.5, 3);
    \draw[dotted, thick] (11.5, 0.5) -- (14,0.5);
    \draw[] (11.5, 3) -- (14, 2.95);
    \draw[] (14, 0.5) -- (14, 2.95);
    \draw[dotted, thick] (10,0) -- (11.5, 0.5);
    \draw[] (10, 2.5) -- (11.5, 3);
    \draw[] (12.5,0) -- (14, 0.5);
    \draw[] (12.5, 2.5) -- (14, 2.95);
    \filldraw[black] (10,0) circle(1pt);
    \filldraw[black] (12.5,2.5) circle(1pt);
    \filldraw[black] (10,2.5) circle(1pt);
    \filldraw[black] (12.5,0) circle(1pt);
    \filldraw[black] (11.5,0.5) circle(1pt);
    \filldraw[black] (14,0.5) circle(1pt);
    \filldraw[black] (11.5,3) circle(1pt);
    \filldraw[black] (14,2.95) circle(1pt);

    \filldraw[color=black!60, fill=white](11.9,1.5) circle (5pt);
    \filldraw[color=black!60, fill=white](11.9,1.5) circle (3pt);
    \filldraw[black](11.9,1.5) circle (1pt);

    \draw[dotted, thick] (11,1.1) -- (12.9,1.9);
    \node[] at (13,2.7) {$x_2$};
    \filldraw[black] (11,1.1) circle(1pt);
    \filldraw[black] (12.9,1.9) circle(1pt);

    \draw[dotted, thick] (9.7, 1.5) -- (14.3,1.5);
    \node[] at (13.5,1.7) {$x_1$};
    \filldraw[black] (9.7,1.5) circle(1pt);
    \filldraw[black] (14.3,1.5) circle(1pt);

    \draw[dotted, thick] (11.9, -0.5) -- (11.9, 3.5);
    \node[] at (12.1,2.1) {$x_3$};
    \filldraw[black] (11.9, -0.5) circle(1pt);
    \filldraw[black] (11.9, 3.5) circle(1pt);
    \node[] at (14.5,2.1) {$\alpha$};
    \end{tikzpicture}
  \caption{CCD for $m=2$ and $m = 3$ }\label{fig:ccd} %\parencite{ahn2015central}}
\end{figure}





%\subsection*{Orthogonal arrays (OA)} \reva{(This paragraph can be deleted?)}
%An orthogonal array is a structured arrangement of experimental runs that ensures each factor level combination occurs with equal frequency. In other words, orthogonal arrays provide a systematic way to design experiments in a balanced manner, allowing for efficient and effective exploration of multiple factors and their interactions \parencite{box1978statistics, wu2011experiments}. While the full, fractional, and central composite designs are described as ``regular'' designs because the effect of each factor is either estimated independently of all others or fully aliased with one of the other factors, orthogonal arrays are described to be ``non-regular'' as they do not possess this property \parencite{burton2022design}. In
%contrast to regular fractional factorial designs where some factorial effects are fully aliased, orthogonal array-based designs can incorporate partial aliasing which could capture the effect of all two-factor interactions using a small number of runs.
%More specifically, an orthogonal array-based design
%$O A\left(n, s_1^{m_1} s_2^{m_2} \ldots s_\gamma^{m_\gamma}, t\right)$ is one that has an $n \times m$ design matrix where $n$ is the number of runs and $m=m_1+\cdots+m_\gamma$.
%The strength of the orthogonal array $O A$ is denoted by $t$, whereby $m_i$ columns have $s_i \geq 2$ levels such that, within any $t$ columns, all possible level combinations occur equally often. Note that $\gamma=1$ corresponds to the case where all factors have the same number of levels and the associated designs are described as symmetrical orthogonal arrays. Designs that have $\gamma > 1$ are described asymmetric or mixed-level orthogonal arrays \parencite{burton2022design}.
%One advantage of OAs over regular fractional factorial designs is that the $OA$ designs are more efficient and can accommodate mixed level factors with a small number of runs \parencite{ding2013use, jaynes2016use, xu2014combining}


\subsection*{Orthogonal array composite designs (OACD)}
Introduced by \textcite{xu2014combining}, an OACD is a class of composite designs based on a two-level factorial design and a three-level {orthogonal array (OA)}. An $\mathrm{OA}$ of $n$ runs, $m$ columns, $s$ levels, and strength $t$, denoted by $\mathrm{OA}\left(n, s^m, t\right)$, is an $n \times m$ matrix in which all $s^t$ level-combinations appear equally often in every $n \times t$ submatrix \parencite{wu2011experiments}. For example, a $2^m$ factorial design can be viewed as $\operatorname{OA}\left(n, 2^m, t\right)$ with $n=2^m$ and $t=m$. Similarly, a three-level OA can be written as $\operatorname{OA}\left(n, 3^m, t\right)$. Thus, an OACD is a composite design which consists of a two-level factorial design as its factorial points, a three-level OA as its additional points, plus any number of center points \parencite{luna2022orthogonal}.

\subsection*{Space filling designs}
While the previously discussed designs utilize sampling points that are at the boundaries of the design space, space filling designs generate samples that are dispersed throughout the multidimensional design space and not just at the boundary of the design space. These designs are important in sampling a surface as they could capture important regions thereby minimizing the bias between the true structure of the surface and the estimated surface from the sampled points \parencite{gardner2006small, giunta2003overview}. They are of various types depending on the approach used to construct them, e.g., sampling-based -- Latin Hypercube designs, distance-based -- maximin designs, and distribution -- based uniform designs \parencite{burton2022design}.

\begin{description}
  \item Latin hypercube designs (LHDs) -- Based on \textcite{mckay1992latin}'s Latin hypercube sampling, it divides the range of each factor into  bins of equal size, where $n$ also corresponds to the number of samples to be generated resulting in a total of $n^m$ combinations where $m$ is the number of factors  being considered. The $n$ samples are then randomly generated such that for all one-dimensional projections, there will be only one sample in each bin.
  \item Maximin distance designs -- Introduced by \textcite{johnson1990minimax}, this design aims at spreading out the design points in the design space by maximizing the minimum  distance between any two design points. It thus tends to place a large proportion of points at the corners and on the boundaries of the design space. Mathematically, this can be formulated as follows. Suppose we want to construct an $n$-run design in $m$ factors. Let the design region be the unit hypercube $\mathcal{X}$ and let the design be $D=\left\{\x_1 \ldots, \x_n\right\}$, where each design point {$\x_{i}$} is in $\mathcal{X}=[0,1]^m$. The maximin design optimizes the function below:
$$
\max _D \min _{i \ne j} d\left(\x_i, \x_j\right),
$$
where $d\left(\x_i, \x_j\right)$ is the  distance between the points $\x_i$ and $\x_j$.
  \item Maximin Latin hypercube designs -- Unlike Latin hypercube designs,  maximin distance designs do not have good projection properties for each factor. \textcite{morris1995exploratory} proposed to overcome this problem by searching for the maximin distance design within the class of Latin hypercube designs. They also proposed to use the following criterion to achieve maximin distance:
\begin{equation}
\min _D\left\{\sum_{i=1}^{n-1} \sum_{j=i+1}^n \frac{1}{d^p\left(\x_i, \x_j\right)}\right\}^{1 / p}
\label{morris_mitchelle}
\end{equation}
where $p > 0$ is chosen large enough, say $p=15$.

  \item Maximum projection (MaxPro) designs  -- Although maximin Latin hypercube designs ensure good space-filling in $m$ dimensions and uniform projections in each dimension, their projection properties in two to $m - 1$ dimensions are not known. By the effect sparsity principle \parencite{wu2011experiments}, only a few factors are expected to be important. To curb this, \textcite{joseph2015maximum} proposed a different criterion:

\begin{equation}
\min _D \psi(D)=\left\{\frac{2}{n(n-1)} \sum_{i=1}^{n-1} \sum_{j=i+1}^n \frac{1}{\prod_{l=1}^m\left(x_{i l}-x_{j l}\right)^2}\right\}^{1 / m} .
\end{equation}
and showed that the design that minimizes $\psi(D)$ tends to maximize its projection capability in all subspaces of factors, and thus named these designs as maximum projection designs.
\end{description}


\section{Modeling}\label{sec:models}
We consider three different frameworks to model the data: (a) Linear Model, (b) Kriging Model and (c) Heterogeneous Gaussian Process (HetGP).

\subsection*{{Linear Model (lm)}}

For $m$ quantitative factors, denoted by $x_1, \ldots, x_m$, a second-order linear model is defined as
\begin{equation}
y=\beta_0+\sum_{i=1}^m \beta_i x_i+\sum_{i=1}^m \beta_{i i} x_i^2+\sum_{i<j} \beta_{i j} x_i x_j+\epsilon
\end{equation}
where $\beta_0, \beta_i, \beta_{i i}$, and $\beta_{i j}$ are the intercept, linear, quadratic, and bilinear (or interaction) terms, respectively, and $\epsilon$ is the error term. This model is simple and provides a straightforward way to model and understand relationships between the response variable and the factors. The main effects are easy to to interpret.  % In R the $lm$ function was used to fit this model.

\subsection*{Kriging Model (km)}
Proposed by South African geostatistician \textcite{krige1951statistical}, Kriging is one of the methods used to interpolate intermediate values, whereby these intermediate values are modeled using Gaussian Process (GP) which is governed by prior co-variances. It provides a probabilistic prediction of the output variable, as well as an estimate of the uncertainty of the prediction \parencite{chevalier2014kriginv}. The kriging predictors interpolating the observations are assumed to be noise-free \parencite{roustant2012dicekriging}. Intermediate interpolated values obtained by Kriging are the best linear unbiased predictors. % There are two forms of fitting a kriging model:  Simple Kriging and Universal Kriging.

%\reva{(not helpful to talk about simple kriging.)}
%\iffalse %
%In simple kriging (SK), $Y(\x)$ is assumed to be the sum of a known deterministic trend function and a centered square-integrable process:
% \begin{equation}
%       Y(\x) = \mu(\x) + Z(\x),
% \end{equation}
% where \(\mu(\x)\) is the trend function and \(Z(\x)\) is a stationary GP with zero mean and covariance function \(\psi\). % that is known %and \(\epsilon \sim N(0,\tau^2)\) is a random error term independent of \(Z(x)\).
% In Universal Kriging (UK), the trend $\mu(\x)$ is known up to a set of linear trend and unknown coefficients. This linear trend is set to be the linear combination of some fixed basis functions $f_i$'s.
% That is
% \begin{equation}
% \mu(\x)=\sum_{i=1}^k \beta_i f_i(\x) .
% \end{equation}\\
% UK consists of deriving the best linear predictions of $Y(\x)$ based on the observations
% while estimating the vector $\beta:=(\beta_1, \dots,\beta_k)^\top$ on the fly. Note that in the specific case where the basis functions reduce to a unique constant function, UK is referred to as ordinary Kriging (OK) \parencite{roustant2012dicekriging}.
% \fi

The Kriging model consists of two parts: a trend and a GP. The trend part is often modeled as a regression on some fixed basis functions. In the specific case where the basis functions reduce to a constant function, it is referred to as ordinary Kriging  \parencite{roustant2012dicekriging}. The general form is as given below
\begin{equation}\label{eq:UK}
      Y(\x) = \sum_{i=1}^k \beta_i f_i(\x) + Z(\x),
\end{equation}
where $f_1, \ldots, f_k$ are $k$ basis functions, $\beta_1, \dots,\beta_k$ are corresponding regression coefficients, and  \(Z(\x)\) is a stationary GP with zero mean and covariance function \(\psi\). % that is known %and \(\epsilon \sim N(0
The covariance function $\psi$ completely defines the behavior of the Gaussian Process $Z(\x)$. It is defined as
\begin{equation}
%\resizebox{0.5\textwidth}{!}{$
\psi\left(\x_{i},\x_{j}\right) =\operatorname{Cov} \left(Z\left(\x_{i}\right), Z\left(\x_{j}\right)\right) = \sigma^{2} \prod_{l=1}^{m} K\left(h_{l} ; \theta_{l}\right),
%$}
\end{equation}
where $\sigma^2$ is the scale parameter called the process variance,
$h_l = |x_{i,l}-x_{j,l}|$, $x_{i,l}$ and $x_{j,l}$ are the $l$th elements of the $i^{th}$ run $\x_i$ and the $j^{th}$ run $\x_j$, and $K(h ; \theta)$ is the correlation function.
%For a stationary GP, the mean and covariance remain constant with respect to time.
The parameters $\theta_l$ chosen for the correlation function $K\left(h_{l};\theta_{l}\right)$  must be positive. Otherwise the correlation function will not be feasible. These parameters are chosen to be physically interpretable in the same unit as the corresponding variables. They are often referred to as the \textit{characteristic length-scales} by \textcite{rasmussen2006gaussian}. Popular correlation functions include Gaussian, Mat\'ern, and power-exponential family correlation functions. The Mat\'ern function with parameter $\nu = 5/2$ is often chosen as the default when fitting kriging models. It is defined as:
\begin{equation}
K(h ; \theta)=\left(1+\frac{\sqrt{5} h}{\theta}+\frac{5 h^{2}}{3 \theta^{2}}\right) \exp \left(-\frac{\sqrt{5} h}{\theta}\right).
\end{equation}
The unknown parameters can be estimated via MLE or cross validation. In R, the \texttt{km} function  in the DiceKriging package was used to fit the kriging model.


\subsection*{{Heteroskedastic Gaussian Process (HetGP)}}

HetGP follows the simplifying assumption in the computer experiments literature in using a mean zero GP, which shifts all of the modeling effort to the covariance structure \parencite{binois2018practical}.
Observation model is given by %\reva{(provide more details on hetGP. what is $r(x)$?)}
\begin{equation}
y_i = y\left(\x_i\right)=f\left(\x_i\right)+\varepsilon_i, \quad\varepsilon_i \sim \mathcal{N}\left(0, r\left(\x_i\right)\right),
\end{equation}
where $f(\x_i)$ is a GP with covariance or kernel $k(\cdot, \cdot)$ and $r(\x_i)$ is the variance of $\epsilon_i$ which depends on $\x_i$. The  kernel $k(\cdot, \cdot)$ is positive definite, with parameterized families such as the Gaussian or Mat\'ern being typical choices. If $r(\x_i)=\tau^2$ is a constant, then the process is homoskedastic.
In matrix notation, the modeling framework just described is equivalent to writing
$$
Y \sim \mathcal{N}\left(\mathbf{0}, \mathbf{K}_n+\boldsymbol{\Sigma}_n\right),
$$
where $\mathbf{K}_n$ is the $n \times n$ matrix with $(i, j)$ coordinate $k\left(\x_i, \x_j\right)$, and $\boldsymbol{\Sigma}_n=\operatorname{Diag}\left(r\left(\x_1\right), \ldots, r\left(\x_n\right)\right)$ is the variance matrix of the  vector of independent noise $\varepsilon_i$.

Given the kernel function $k(\cdot, \cdot)$ and data $\y=(y_1,\ldots,y_n)^{\top}$, multivariate normal (MVN) conditional identities provide a predictive distribution at site $\x: Y(\x) \mid \y$, which is Gaussian with parameters
$$
\begin{aligned}
\mu(\x) & =\mathbb{E}(Y(\x) \mid \y)=\mathbf{k}(\x)^{\top}\left(\mathbf{K}_n+\mathbf{\Sigma}_n\right)^{-1} \y,  \\
\sigma^2(\x) & =\mathbb{V} \operatorname{ar}(Y(\x) \mid \y)=k(\x, \x)+r(\x)-\mathbf{k}(\x)^{\top}\left(\mathbf{K}_n+\mathbf{\Sigma}_n\right)^{-1} \mathbf{k}(\x) ,
\end{aligned}
$$
where $\mathbf{k}(\x)=\left(k\left(\x, \x_1\right), \ldots, k\left(\x, \x_n\right)\right)^{\top}$.
In R, the \texttt{mleHetGP} function in hetGP package is used to fit this model with the default Gaussian kernel.


\section{The Data Generation Process}
%\reva{(I've made many changes in this section. Revise as needed.)}

We aim to obtain optimal DE hyperparameter settings that can be used to generate UPDs. %The objective function is the uniform projection criterion. %This is then minimized and the minimum returned as the response value.

\subsection*{The Objective Function: Uniform Projection Design Criterion}
Proposed by \textcite{sun2019uniform}, the uniform projection criterion solely focuses on  two-dimensional projections. This is due to two factor interactions being more important than three-factor or higher-order interactions. The motivating idea was that although designs with low discrepancy have good uniformity in the full-dimensional space, they can have bad projections in lower dimensional spaces, which is undesirable when only a few factors are active. Thus designs with better projection properties are preferred. \textcite{sun2019uniform} argued that the uniform projection designs scatter points uniformly in all dimensions and have good space-filling properties in terms of distance, uniformity and orthogonality.

The uniform projection design criterion is defined using the centered $L_2$-discrepancy.
For an $n \times m$ design $D=\left(x_{i k}\right)$ with $s$ levels from $\{0, 1, \ldots, s-1 \}$, its (squared) centered $L_2$-discrepancy is defined as
$$
\begin{aligned}
  \mathrm{CD}(D)= & \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n
  \prod_{k=1}^m\left(1+\frac{1}{2}\left|z_{i k}\right| +
  \frac{1}{2}\left|z_{j k}\right| -
  \frac{1}{2}\left|z_{i k}-z_{j k}\right|\right) \\
  & -\frac{2}{n} \sum_{i=1}^n \prod_{k=1}^m\left(1+\frac{1}{2}
  \left|z_{i k}\right|-\frac{1}{2}\left|z_{i k}\right|^2\right)+
  \left(\frac{13}{12}\right)^m,
  \end{aligned}
$$
where $z_{i k}=\left(2 x_{i k}-s+1\right) /(2 s)$. Then the uniform projection criterion is to minimize
\begin{equation}
  \phi(D)=\frac{2}{m(m-1)} \sum_{|u|=2} \mathrm{CD}\left(D_u\right),
  \label{upd}
\end{equation}
where $u$ is a subset of $\{1,2, \ldots, m\},|u|$ denotes the cardinality of $u$ and $D_u$ is the projected design of $D$ onto dimensions indexed by the elements of $u$. The $\phi(D)$ is the average centered $L_2$-discrepancy values of all two-dimensional projections of $D$.

We implemented the DE and uniform projection criterion in the package \texttt{UniPro}. % \texttt{Meta4Design}.
The following code generates an $n\times m$ UPD with $s$ levels
<<eval = FALSE, echo=TRUE, fig=false>>=
UniPro(n, m, s, NP, itermax, pMut, pCR, pGBest, seed)
@
where NP, itermax, pMut, pCR and pGBest are DE hyperparameters described in Section 2, and seed is an optional seed for random number generators that ensures reproducibility.

As the task of design generation is quite complex, only 3 design sizes are considered.  A UPD of size $30\times3$ is considered as a small and easy task, $50\times5$ as a medium task and $70\times 7$ as a large and difficult task. We only consider the construction of designs with $s=n$ so that the resulting UPD is an LHD.

\subsection*{Training and testing data}

Designs discussed in Section \ref{sec:DE.designs} are used to determine the parameter settings for the DE algorithm hyperparameters. Specifically, we construct five designs: a CCD with 43 runs (\verb|ccd3_43|), an OACD with 50 runs (\verb|oacd3_50|), 50-run random LHD, 50-run maximin LHD, and 50-run maxpro LHD. All designs have five factors, one for each hyperparameter. Each run corresponds to a setting of the five hyperparameters. The CCD and OACD have 3 levels while the rest have 50 levels. The levels are linearly interpolated within the minimum and maximum factor values for each hyperparameter. Generating the test data from the $3^5$ factorial, 243-run random LHD, and the  $4^5$ factorial seems good enough. This is because the random LHD enjoys the maximum space-filling property in all one dimensions, while the $3^5$ and $4^5$ factorial designs cover the entire 5-dimensional input space in a uniform fashion. % \parencite{shi2023evaluating}.

Given the target design size $(n \times m)$ and a setting of the five hyperparameters, we run the \texttt{UniPro} function to generate an $n \times m$ UPD and the resulting $\phi(D)$ value defined in  \eqref{upd}. This is recorded as the response value for that particular hyperparameter setting and target design size. For each setting, the DE algorithm is replicated ten times yielding ten replicates for the response. These are then aggregated to obtain the mean and the standard deviation of the response.
Thus we obtain five training datasets for each target size.

%Due to the difference in the structure of the designs, various factor levels were employed, with each hyperparameter domain remaining as previously stated while the levels within the minimum and maximum factor values were linearly interpolated.

%For the training data, one of the designs amongst a CCD with 43 runs (\verb|ccd3_43|), an OACD with 50 runs (\verb|oacd3_50|), 50-run LHD, maximin and maxpro was used to determine the hyperparameter settings. For each parameter setting combination, ten $\phi(.)$ values for a target UPD were computed. These were then avaraged to obtain the response value for the said parameter combination.

The same procedure is taken to generate the testing dataset with the exception that the designs used for the hyperparameter setting combinations being a $3^5$ full factorial design, a random LHD with 243 runs, a combination of these two, and a $4^5$ full factorial design.

Density plots of the response for the testing and training data are presented in Figure \ref{fig:density} for the target size $50\times 5$.  All of the distributions are skewed to the right. For a $50\times5$ UPD all the training designs lead to similar minimum $\phi(.)$ values, around 0.17, whereas different types of designs lead to different maximum $\phi(.)$ values. Indeed, all space filling designs have  maximum $\phi(.)$ values around 0.28, while the factorial designs and the hybrid design have a maximum $\phi(.)$ values around 0.34. The narrower range of the $\phi(.)$ values suggests that the space filling designs do not explore the entire space of hyperparameters.
%The space filling designs have a smaller range of the $\phi(.)$ values than the factorial designs. The maximum

\begin{figure}%[!ht]
    \centering

<<include=false, label=pdfs/density>>=
source("R/plotting.R")
density.plot(hist = FALSE)
@
\includegraphics{chapters/DE/pdfs/density}
  \caption{Density plots of the $\phi(D)$ values with target size $50\times5$}
    \label{fig:density}
\end{figure}


\subsection*{{Model evaluation}}
For each training dataset, we fit the three models descibed in Section \ref{sec:models} and test on the four testing datasets. Designs are evaluated by considering their ability to collect informative data for building a statistical model that specifies the relationship between the response and the hyperparameters, which is measured by the test root mean squared error (RMSE). The correlation ($\rho$) between the response and the predicted together with the RMSE are reported.

%\clearpage
\section{Results and Analysis}

%\reva{(Merge Table \ref{tab:tab1} and Table \ref{tab:tab2} into one table)}
%There are various striking observations made from the analysis done. Tables 1-2 are results for the class considered as small. That is, the target size of the generated Uniform projection design is $30\times 3$.


<<results=tex, fig=false>>=
tabs1_2 = tables("data/up30x3", 2)
tabs1_2
@


\begin{figure}%[!ht]
\centering
<<label=pdfs/barplots1, include=false>>=
generate_boxplots("data/up30x3")
@
\includegraphics[height=5in, width=6in]{chapters/DE/pdfs/barplots1}
\caption{Comparison of RMSE with target size $30\times3$}
\label{barplots1}
\end{figure}


{Table \ref{tab:tab1}(a)(b) and Figure \ref{barplots1}(a)(b) present comparison of designs and model evaluations with target size $30\times 3$ for testing the two 243-run data sets.
%There are various striking observations made from the analysis done. Tables 1-2 are results for the class considered as small. That is, the target size of the generated Uniform projection design is $30\times 3$.
One striking observation is that the performance of the training data set depends on the nature of the testing data set. The  composite designs, CCD and OACD, seem to be better when tested on the $3^5$ FFD while the space filling designs (random LHD, maximin LHD, and maxpro LHD) did better when tested on the 243-run random LHD.
As this does not give a general idea as to which designs might perform better in general, we invoke the combined data with 486 (runs $3^5$ FFD and 243-run LHD) and the $4^5$ FFD as the testing dataset.
Here we see that the composite designs perform better than the space filling designs; see Table \ref{tab:tab1}(c)(d) and Figure \ref{barplots1}(c)(d). The 50-run random LHD performed the worst in terms of correlation regardless of the testing data. The correlation is strikingly low whereas the RMSE is high. This might be due to randomness, but it does show the weakness of the random LHD.} %, but the result is not replicated in the $50\times 5$ and $70\times 7$ design sizes.

One bizarre observation from Table \ref{tab:tab1}(b) is the correlation of $0.03$ when using the CCD as the training design and testing it on the 243-run random LHD. This value is strikingly lower than any other values given in Table \ref{tab:tab1}. No apparent reason could be deduced as to why this is so. Multiple replications indicated that this is not an error.  From all the results, we can deduce the robustness of OACD over CCD. This gives a reason to use OACD for the hyperparameter initialization.




%When using the training as the testing dataset, the kriging model gives a correlation of 1 and RMSE of 0. This is expected since the kriging perfectly interpolate the known values at the training locations while assuming a stationary covariance.  This assumption cannot be proven as the covariance between two points in the DE hyperparameter surface structure depends cannot be shown to depend only on the distance or spatial lag between those points, and not on their specific locations within the domain. Due to this, the heteroskedastic gaussian process is preferred.


%In addition, the performance of the CCD and OACD training datasets when tested on the $3^5$ factorial design was better than when tested on the $4^5$ factorial design.  This could be attributed to the fact that there are $4$ levels in the testing dataset yet only $3$ in the training dataset, thus there are ``holes" in the experimental space because the testing phase introduces two levels that have not been explored during training. These gaps may lead to uncertainties in predicting the response at the unexplored level and thereby get poor results as compared to the $3^5$ factorial.


With regards to the models, there seems to be no striking observation to be made as to whether one fitting method performs better than the other two, with  exception for one $30 \times 3$ case when the kriging model fitting to the CCD training data had a much higher RMSE value than the other cases.
%Although using the $3^5$ full factorial design as the training dataset and then testing on the combined dataset, it can be noted that the Heteroskedastic Gaussian model is significantly better than both kriging and linear model. As HetGP enjoys more data points, this might be the reason of its better performance. Thus reasonable to conclude that Heterogeneous Gaussian model should be preferred even when the linear model does result in almost similar results.


The three models have quite different assumptions. The linear model assumes a polynomial trend and independent random errors with homoskedastic variance. The Kriging model assumes a stationary covariance structure, that is, the covariance between two points in the DE hyperparameter surface structure depends only on the distance or spatial lag between those points, and not on their specific locations within the domain. The HetGP model assumes a heteroskedastic variance-covariance structure. For the DE algorithm, the homoskedastic and stationary assumptions are questionable. Due to this, the HetGP model is preferred to the linear model and the kriging model. However, the linear model is easy to interpret and fits as well as the HetGP model, and from the results, there is no striking difference between the two. We use the linear model to determine factor importance and optimal hyperparameter settings.

%. On the other hand, space filling designs do not capture well the vertex/boundary information.


Tables \ref{tab:tab2}-\ref{tab:tab3} and
Figures \ref{barplots2}-\ref{barplots3} in the Appendix show results when the target design sizes are $50\times 5$ and $70\times7$, respectively.
Looking at the results, apart from the random LHD with 50 runs, previously stated observations are upheld. % The composite designs perform better when tested on the $3^5$ FFD while the space filling designs perform better when tested on the 243-run LHD. When tested on the  486-run combined data and the $4^5$ FFD, the composite designs perform better than the space filling designs, although the difference tends to be small as the values are very close. In addition, there is no clear difference between the performances of the three models.
% indicating that perhaps in large designs, there won't be a distinction between the space filling designs and the composite designs. %This is probably because as the dimension increases, most of the space within the cube is empty, since everything is almost pushed to the boundary, hence both the space filling designs and the factorial composite designs would be expected to yield almost similar prediction results.


A natural question is why composite designs perform better than the space filling designs. We perceive that the hyperparameters at the boundaries lead to some extreme cases in this experiment and the composite  designs do capture this phenomena while the space filling designs do not. Figure \ref{fig:distance} presents the histograms of the distances from design points to the design center for all the designs, where each column is rescaled to $[-1, 1]$ and  the euclidean distance from each point to the center of the design is calculated. This gives an insight as to why the composite designs, OACD and CCD, tend to perform better than the space filling designs. This is because the composite designs tend to capture information lying at the boundaries compared to the space filling designs which tend to capture the information lying at the center of the design. This is confirmed by the notion that three of the hyperparameters tend to be optimized around their highest level as discussed in the next section.



\begin{figure}%[!ht]
    \centering
    %\includegraphics[scale = 0.5]{images/distance.png}
<<label=pdfs/histogram, include=false>>=
a=capture.output(density.plot())
@
    \includegraphics{chapters/DE/pdfs/histogram}
    \caption{Histogram of the distances from  design points to the design center}
    \label{fig:distance}
\end{figure}


%\clearpage
\section{Factor Importance and Optimal Settings}
%\reva{(I've made many changes in this section. Revise as needed.)}
The results obtain call for a deeper look into the model and how each factor is involved in the surface approximation. This enables us to have a better picture of the surface generated by the DE hyperparameters.

 % Variable importance assessment can be divided, generaly, into two groups: model-specific and model-agnostic/model free \parencite{biecek2021explanatory}. For linear models and many other types of models, there are methods of assessing explanatory variable's importance that exploit particular elements of the structure of the model. These are model-specific methods.
 % For instance, for linear models, the value of the normalized regression coefficient or its corresponding p-value could be used as the variable-importance measure. For tree-based ensembles, such a measure may be based on the use of a particular variable in particular trees. A great example in this respect is the variable-importance measure based on out-of-bag data for a random forest model \parencite{breiman2001random}, but there are also other approaches like methods implemented in the XgboostExplainer package \parencite{foster2017xgboostexplainer} for gradient boosting and randomForestExplainer \parencite{paluszynska2017random} for random forest.
 %

<<fig=false, results=tex>>=
auto.lm("data/up30x3")
@

Table \ref{table:coefficients} shows the estimated coefficients of the second-order models based on the four different training datasets: CCD, OACD, maximin LHD, and maxpro LHD, for the target size $30 \times 3$.
Looking at the various models above, the model obtained using the maxpro LHD as the training data is the worst performing model. It has the lowest adjusted $R^2$ of only $0.54$. This model does not capture important main effects. For example, it indicates that the number of iterations (itermax) for optimization is not significant. Yet this is well known to be important. On the other hand, the model obtained by using OACD as the training dataset performs the best. It has an adjusted $R^2$ of $0.87$ and captures important main effects and interactions. In addition, CCD might be a little worse than OACD because of the fewer number of points $(43)$ used for training compared to the other models which used $50$ points.
The model from the CCD does not identify any of the quadratic effects to be significant while the other models do.

Looking at the corresponding p-values from the models obtained, it is evident that all five hyperparameters are important. The main effects of three hyperparameters (NP, itermax and pGBest) are very significant, whereas the several interactions involving one of the other two hyperparameters (pMut and pCR) are also very significant.
The probability of using the global best (pGBest) is quite important because its main effect is highly significant in all the models in Table \ref{table:coefficients}. It can also be inferred that the maximum number of iterations (itermax) and the population size (NP) are important. The linear models indicate that to minimize the objective function, it is ideal to use a larger pGBest, increase the population size (NP) and increase the number of iterations (itermax). The population size and the number of iterations could be constrained by the available budget.

With regards to interactions, all significant interaction terms involve either pMut or pGBest. The interaction of pMut and pGBest is negative. On the other hand, the interaction between pMut and pCR is positive. As this is a minimization problem, this positive interaction indicate that the two variables need to be either at their low levels or one has to be low while the other high. Both should not be at their high levels.


\begin{figure}
\centering
<<label=pdfs/interactions, include = false>>=
my_interaction_plot_base2()
@
\includegraphics[width=0.95\textwidth]{chapters/DE/pdfs/interactions}
\caption{Interaction plots involving pMut based on the $4^5$ FFD and target size $30\times3$.}
\label{interaction}
\end{figure}


%-------------------------------------------------------
To have a better understanding of the interactions, we examine some interaction plots from the full $4^5$  factorial, shown in Figure \ref{interaction}. Here, the number of population (NP), number of iterations (itermax) and the probability of using global best (pGBest) are held at their highest levels, i.e., 100, 1500 and 0.95, respectively. These results are consistent with the second order models in Table \ref{table:coefficients}, where NP, itermax and pGBest are all negative and significant. Thus they should be set at the highest level to minimize the response values. These results are consistent for other target design sizes. For pMut and pCR, their interaction is more complicated as it is positive and thus a further analysis is called for to determine the preferred levels to set these parameters. We use contour plots to visualize the two parameters when the other three are held at their highest level.

%\reva{(ToDO: look at interaction plots for pMut:pCR, pMut:pGBest, pMut:itermax, pMut:NP for $4^5$ FFD. Based on the interaction plots, identify good hyperparameter settings and confirm the findings from the OACD and the second-order model.)}

\begin{figure}
\centering
<<label=pdfs/contours, include=false>>=
a <- f_contours(c("ccd3_43.csv", "oacd3_50.csv"))
@
\includegraphics[height=10cm, width=\textwidth]{chapters/DE/pdfs/contours}
\caption{Contour plots of pMut and pCR while fixing other hyperparameters at high levels. Top row uses CCD as the training data; bottom row uses OACD as training data.}
\label{fig:contours}
\end{figure}

Figure \ref{fig:contours} shows the contour plots of pMut and pCR while fixing NP=100, itermax=1500 and pGBest=0.95. From the contour plot based on CCD and OACD training data set, and target size $30\times 3$, we note that the response value could be minimized by taking a values over the off diagonal. Either a smaller pMut and larger pCR or a larger pMut and a smaller pCR. This is also the case for target sizes $50\times5$ and $70\times 7$. Searching the optimal setting is achieved by  varing pMut from $0.05, 0.15, \ldots, 0.95$ with fixed parameters $pCR=1-pMut$. For each setting of pMut and pCR, the DE algorithm is run $100$ times to construct $100$ designs while fixing $NP=100$, $itermax=1500$ and $pGBest=0.95$. The average response values are then calculated across these runs, allowing us to identify the optimal settings of $pMut$ and $pCR$ for each target size. This approach results in the folllowing optimal settings of $(pMut, pCR)$: $(0.95, 0.05)$, $(0.25, 0.75)$, and $(0.15, 0.85)$ for target size $30\times3$, $50\times5$ and $70\times7$ respectively



The optimal settings are compared with two DE variants, DE1 and DE4, described by \textcite{stokes2023metaheuristic}. DE1 uses only the global best solution (\( pGBest = 1 \)), while DE4 is a hybrid approach with \( GBest = 0.5 \), where the global best, the current agent, and a random agent are selected with probabilities of \( 0.5 \), \( 0.25 \), and \( 0.25 \), respectively, for each column independently. Both DE1 and DE4 utilize fixed values of \( pMut = 0.1 \) and \( pCR = 0.5 \).  

For the comparison, the optimal settings for \( pMut\) and \( pCR \), determined for each target size, are used while maintaining \( pGBest = 0.95 \). In all cases, \( NP = 100 \) and \( itermax = 1500 \) are set, and the DE algorithm is executed 100 times to construct 100 designs for each method. Figure \ref{fig:three graphs} presents boxplots of the \( \phi(\cdot) \) values of these designs. The results demonstrate that DE with optimal settings (DEoptim) produces significantly better uniform projection designs compared to the two variants provided by \textcite{stokes2023metaheuristic}, confirming the effectiveness of hyperparameter tuning.




\begin{figure}[!h]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
<<pdfs/boxplots2, include=false>>=
comparison(30, 3, 100)
@
         \includegraphics[width=\textwidth]{chapters/DE/pdfs/boxplots2}
         \caption{$30\times 3$ as target}
         %\label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
<<pdfs/boxplots3, include=false>>=
comparison(50, 5, 100)
@
         \includegraphics[width=\textwidth]{chapters/DE/pdfs/boxplots3}
         \caption{$50\times 5$ as target}
         %\label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
<<pdfs/boxplots4, include=false>>=
comparison(70, 7, 100)
@
         \includegraphics[width=\textwidth]{chapters/DE/pdfs/boxplots4}
         \caption{$70\times 7$ as target}
         %\label{fig:five over x}
     \end{subfigure}
        \caption{Performance of the DE algorithms under three setttings: DE1, DE4 and DEoptim (optimal settings)}
        \label{fig:three graphs}
\end{figure}




\section{Conclusion}
This paper compared small designs in exploring the response surface of a DE algorithm yperparameters. Five numerical hyperparameters were considered:  population size, the number of maximum iterations, probability of crossover, probability of mutation, and probability of using the global best for mutation. Various composite designs and space-filling designs for selecting combinations of these hyperparameters were also examined. The performance of a design was evaluated via building a second order model, a kriging model and a heterogeneous GP model. The performance was measured in terms of testing RMSEs and correlation. The comparison was made based on data simulated using the uniform projection criterion. Under the settings considered, the comparison demonstrates that OACD and CCD are the better choices over space-filling designs for exploring the response surface of the DE algorithm hyperparameters. In addition, the second-order model is simple and works just as well as the Kriging model and the heterogeneous GP model in this situation. The importance of tuning the DE algorithm is demonstrated and a simple strategy on determining optimal hyperparameter settings for  constructing UPDs with different target sizes is provided.

While the primary goal of optimizing hyperparameters is to find an optimal hyperparameter combination that maximizes the overall performance of a learning algorithm, the paper additionally examines the impact of different design configurations on the effectiveness of hyperparameter tuning. The insights gained are subsequently used to select the best hyperparameters, which are then applied by the DE algorithm to construct UPDs.

\clearpage
%\pagebreak
\section*{Appendix: Additional tables and figures for target sizes $50\times5$ and $70\times7$}



\begin{figure}[!ht]
\centering
<<pdfs/barplots2, include=false>>=
generate_boxplots("data/up50x5")
@
\includegraphics[height=5in, width=6in]{chapters/DE/pdfs/barplots2}
\caption{Comparison of RMSE with target size $50\times5$}
\label{barplots2}
\end{figure}


<<results=tex, fig=false>>=
tabs3_4 = tables("data/up50x5", 2)
tabs3_4
@


\begin{figure}%[!ht]
\centering
<<pdfs/barplots3, include=false>>=
generate_boxplots("data/up70x7")
@
\includegraphics[height=5in, width=6in]{chapters/DE/pdfs/barplots3}
\caption{Comparison of RMSE with target size $70\times7$}
\label{barplots3}
\end{figure}



<<results=tex, fig=false>>=
tabs5_6 = tables("data/up70x7", 2)
tabs5_6
@

%\printbibliography


<<fig=false,echo=false>>=
setwd(dr)
@

%\end{document}
